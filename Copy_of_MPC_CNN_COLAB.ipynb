{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of MPC_CNN_COLAB.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cateberry/dissertation/blob/dissertation/Copy_of_MPC_CNN_COLAB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZI-I0ftonRZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "df9aa7d0-d062-4abf-f730-c3e3f9eb90a6"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "ROOT = \"/content/drive\"     # default location for the drive\n",
        "print(ROOT)                 # print content of ROOT (Optional)\n",
        "\n",
        "drive.mount(ROOT) "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E34ijRNvF13z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a2016bda-7ed3-4140-c647-f5701c66e190"
      },
      "source": [
        "from os.path import join  \n",
        "\n",
        "# path to your project on Google Drive\n",
        "MY_GOOGLE_DRIVE_PATH = 'My Drive/dissertation' \n",
        "# replace with your Github username \n",
        "GIT_USERNAME = \"cateberry\" \n",
        "# definitely replace with your\n",
        "GIT_TOKEN = \"7dade44469ddae48962f5d81a3b257589ed1b2c9\"\n",
        "# Replace with your github repository in this case we want \n",
        "# to clone deep-learning-v2-pytorch repository\n",
        "GIT_REPOSITORY = \"dissertation\"\n",
        "\n",
        "PROJECT_PATH = join(ROOT, MY_GOOGLE_DRIVE_PATH)\n",
        "\n",
        "# It's good to print out the value if you are not sure \n",
        "print(\"PROJECT_PATH: \", PROJECT_PATH)   \n",
        "\n",
        "# In case we haven't created the folder already; we will create a folder in the project path \n",
        "# !mkdir \"{PROJECT_PATH}\"\n",
        "\n",
        "#GIT_PATH = \"https://{GIT_TOKEN}@github.com/{GIT_USERNAME}/{GIT_REPOSITORY}.git\" this return 400 Bad Request for me\n",
        "GIT_PATH = \"https://\" + GIT_TOKEN + \"@github.com/\" + GIT_USERNAME + \"/\" + GIT_REPOSITORY + \".git\"\n",
        "print(\"GIT_PATH: \", GIT_PATH)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PROJECT_PATH:  /content/drive/My Drive/dissertation\n",
            "GIT_PATH:  https://7dade44469ddae48962f5d81a3b257589ed1b2c9@github.com/cateberry/dissertation.git\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eE8PdFXEGfC8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "39671ce2-6108-4cb4-cad0-e32e8cfc65d1"
      },
      "source": [
        "%cd \"{PROJECT_PATH}\"\n",
        "!git clone \"{GIT_PATH}\""
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/dissertation\n",
            "fatal: destination path 'dissertation' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFcuJux1hRgp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "outputId": "5e24cdcc-9c25-43ed-ddc8-a9179961b3f3"
      },
      "source": [
        "%cd /content/drive/My\\ Drive/dissertation\n",
        "!python setup.py build_ext --inplace"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/dissertation/dissertation\n",
            "Compiling image_analysis/im2col/im2col_cython_float.pyx because it changed.\n",
            "Compiling image_analysis/im2col/im2col_cython_object.pyx because it changed.\n",
            "[1/2] Cythonizing image_analysis/im2col/im2col_cython_float.pyx\n",
            "/usr/local/lib/python3.6/dist-packages/Cython/Compiler/Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: /content/drive/My Drive/dissertation/dissertation/image_analysis/im2col/im2col_cython_float.pyx\n",
            "  tree = Parsing.p_module(s, pxd, full_module_name)\n",
            "[2/2] Cythonizing image_analysis/im2col/im2col_cython_object.pyx\n",
            "/usr/local/lib/python3.6/dist-packages/Cython/Compiler/Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: /content/drive/My Drive/dissertation/dissertation/image_analysis/im2col/im2col_cython_object.pyx\n",
            "  tree = Parsing.p_module(s, pxd, full_module_name)\n",
            "running build_ext\n",
            "building 'image_analysis.im2col.im2col_cython_float' extension\n",
            "creating build\n",
            "creating build/temp.linux-x86_64-3.6\n",
            "creating build/temp.linux-x86_64-3.6/image_analysis\n",
            "creating build/temp.linux-x86_64-3.6/image_analysis/im2col\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/numpy/core/include -I/usr/include/python3.6m -c image_analysis/im2col/im2col_cython_float.c -o build/temp.linux-x86_64-3.6/image_analysis/im2col/im2col_cython_float.o\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/numpy/core/include/numpy/ndarraytypes.h:1832:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/numpy/core/include/numpy/ndarrayobject.h:12\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/numpy/core/include/numpy/arrayobject.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kimage_analysis/im2col/im2col_cython_float.c:629\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K#warning \"Using deprecated NumPy API, disable it with \" \"#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [\u001b[01;35m\u001b[K-Wcpp\u001b[m\u001b[K]\n",
            " #\u001b[01;35m\u001b[Kwarning\u001b[m\u001b[K \"Using deprecated NumPy API, disable it with \" \\\n",
            "  \u001b[01;35m\u001b[K^~~~~~~\u001b[m\u001b[K\n",
            "x86_64-linux-gnu-gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/image_analysis/im2col/im2col_cython_float.o -o /content/drive/My Drive/dissertation/dissertation/image_analysis/im2col/im2col_cython_float.cpython-36m-x86_64-linux-gnu.so\n",
            "building 'image_analysis.im2col.im2col_cython_object' extension\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/numpy/core/include -I/usr/include/python3.6m -c image_analysis/im2col/im2col_cython_object.c -o build/temp.linux-x86_64-3.6/image_analysis/im2col/im2col_cython_object.o\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/numpy/core/include/numpy/ndarraytypes.h:1832:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/numpy/core/include/numpy/ndarrayobject.h:12\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/numpy/core/include/numpy/arrayobject.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kimage_analysis/im2col/im2col_cython_object.c:629\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K#warning \"Using deprecated NumPy API, disable it with \" \"#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [\u001b[01;35m\u001b[K-Wcpp\u001b[m\u001b[K]\n",
            " #\u001b[01;35m\u001b[Kwarning\u001b[m\u001b[K \"Using deprecated NumPy API, disable it with \" \\\n",
            "  \u001b[01;35m\u001b[K^~~~~~~\u001b[m\u001b[K\n",
            "x86_64-linux-gnu-gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/image_analysis/im2col/im2col_cython_object.o -o /content/drive/My Drive/dissertation/dissertation/image_analysis/im2col/im2col_cython_object.cpython-36m-x86_64-linux-gnu.so\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGQaiMceYXtz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "1aecf86f-0454-47f0-a5cf-ab3c39f430fa"
      },
      "source": [
        "%cd /content/drive/My\\ Drive/dissertation/dissertation/image_analysis\n",
        "%ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/dissertation/dissertation/image_analysis\n",
            "'Base MPC implementation.py'   'MPC_CNN_COLAB (1).ipynb'\n",
            " Convnet.py                     MPC_CNN_COLAB.ipynb\n",
            "'Copy of MPC_CNN_COLAB.ipynb'  'MPC CNN.py'\n",
            " \u001b[0m\u001b[01;34mim2col\u001b[0m/                        \u001b[01;34mpond\u001b[0m/\n",
            " __init__.py                   'Private Image Analysis.ipynb'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wb26w79iJCp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "outputId": "9e5b947f-c808-4858-9cce-98f751a83868"
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "import time\n",
        "import pond\n",
        "from pond.tensor import NativeTensor, PublicEncodedTensor, PrivateEncodedTensor\n",
        "from pond.nn import Dense, Relu, Reveal, CrossEntropy, SoftmaxStable, Sequential, DataLoader, Conv2D, \\\n",
        "    AveragePooling2D, Flatten, BatchNorm, ReluNormal, ReluGalois, Conv2DQuant, DenseQuant\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# set errors error behaviour for overflow/underflow\n",
        "_ = np.seterr(over='raise')\n",
        "_ = np.seterr(under='raise')\n",
        "_ = np.seterr(invalid='raise')\n",
        "\n",
        "# %%\n",
        "\"\"\"\n",
        "- Load data from Keras\n",
        "- Split into training and test data\n",
        "- Normalise to [0,1] (greyscale images with values in [0,255])\n",
        "- Transform targets to one-hot representation\n",
        "\"\"\"\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "x_train = x_train.astype(np.uint8)\n",
        "x_test = x_test.astype(np.uint8)\n",
        "x_train = x_train[:, np.newaxis, :, :] / 255.0\n",
        "x_test = x_test[:, np.newaxis, :, :] / 255.0\n",
        "# x_train = x_train[:, np.newaxis, :, :]\n",
        "# x_test = x_test[:, np.newaxis, :, :]\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "\"\"\"\n",
        "Split into train-test-val\n",
        "Train in usual way using train and val\n",
        "Write separate function for testing on test set (completely unseen)\n",
        "Purpose of testing function is for evaluating quantized network\n",
        "Compare testing of quantized network with non-quantized network \n",
        "Need a way to save the parameters of the trained MPC network \n",
        "\"\"\"\n",
        "\n",
        "# Size of pooling area for max pooling\n",
        "# pool_size = (2, 2)\n",
        "# Convolution kernel size (kernel_height, kernel_width, input_channels, num_filters)\n",
        "# kernel_size = (5, 5, 1, 16)\n",
        "\n",
        "# convnet_shallow_gal = Sequential([\n",
        "#     #Conv2D((3, 3, 1, 32), strides=1, padding=1, filter_init=lambda shp: np.random.normal(scale=0.1, size=shp)),\n",
        "#     Conv2DQuant((3, 3, 1, 32), strides=1, padding=1, filter_init=lambda shp: np.random.normal(scale=0.1, size=shp)),\n",
        "#     BatchNorm(),\n",
        "#     ReluGalois(order=4, mu=0.0, sigma=1.0),     # TODO: investigate overflow error\n",
        "#     AveragePooling2D(pool_size=(2, 2)),\n",
        "#     Flatten(),\n",
        "#     # Dense(10, 6272),  # 3136 5408 6272\n",
        "#     DenseQuant(10, 6272),\n",
        "#     Reveal(),\n",
        "#     SoftmaxStable()\n",
        "# ])\n",
        "\n",
        "convnet_shallow = Sequential([\n",
        "    Conv2D((3, 3, 1, 32), strides=1, padding=1, filter_init=lambda shp: np.random.normal(scale=0.1, size=shp)),\n",
        "    BatchNorm(),\n",
        "    ReluNormal(order=4, mu=0.0, sigma=1.0, approx_type='lagrange-uniform'), # approx_type='taylor'),\n",
        "    # Relu(order=4),\n",
        "    AveragePooling2D(pool_size=(2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(10, 6272),  # 3136 5408 6272\n",
        "    Reveal(),\n",
        "    SoftmaxStable()\n",
        "])\n",
        "\n",
        "\n",
        "def accuracy(classifier, x, y, verbose=0, wrapper=NativeTensor):\n",
        "    predicted_classes = classifier \\\n",
        "        .predict(DataLoader(x, wrapper), verbose=verbose).reveal() \\\n",
        "        .argmax(axis=1)\n",
        "\n",
        "    correct_classes = NativeTensor(y) \\\n",
        "        .argmax(axis=1)\n",
        "\n",
        "    matches = predicted_classes.unwrap() == correct_classes.unwrap()\n",
        "    return sum(matches) / len(matches)\n",
        "\n",
        "\n",
        "# %%\n",
        "\"\"\"\n",
        "Train on different types of Tensor\n",
        "\"\"\"\n",
        "# NativeTensor (like plaintext)\n",
        "x_train = x_train[:512]\n",
        "y_train = y_train[:512]\n",
        "x_test = x_test[:128]\n",
        "y_test = y_test[:128]\n",
        "\n",
        "tensortype = PrivateEncodedTensor  # TODO: Change back to NativeTensor\n",
        "batch_size = 32\n",
        "input_shape = [batch_size] + list(x_train.shape[1:])\n",
        "epochs = 3\n",
        "learning_rate = 0.01\n",
        "\n",
        "convnet_shallow.initialize(initializer=tensortype, input_shape=input_shape)\n",
        "\n",
        "start = time.time()\n",
        "convnet_shallow.fit(\n",
        "    x_train=DataLoader(x_train, wrapper=tensortype),\n",
        "    y_train=DataLoader(y_train, wrapper=tensortype),\n",
        "    x_valid=DataLoader(x_test, wrapper=tensortype),\n",
        "    y_valid=DataLoader(y_test, wrapper=tensortype),\n",
        "    loss=CrossEntropy(),\n",
        "    epochs=epochs,\n",
        "    batch_size=batch_size,\n",
        "    verbose=1,\n",
        "    learning_rate=learning_rate\n",
        ")\n",
        "end = time.time()\n",
        "time_taken = end - start\n",
        "\n",
        "print(\"Elapsed time: \", time_taken)\n",
        "\n",
        "raise Exception()\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(-0.5*x - 0.5)*(-0.25*x + 0.25)*(-0.166666666666667*x + 0.5), (-0.5*x + 0.5)*(-0.25*x + 0.75)*(0.5*x + 1.5), (-0.5*x + 1.5)*(0.25*x + 0.75)*(0.5*x + 0.5), (0.166666666666667*x + 0.5)*(0.25*x + 0.25)*(0.5*x - 0.5)] [-3.0, -1.0, 1.0, 3.0]\n",
            "[0.375000000000000 0.125000000000000 0.500000000000000\n",
            " -1.17961196366423e-16]\n",
            "2020-08-19 10:04:08.760188 Epoch 1/3\n",
            "512/512 [============================>..] - ETA: 0:04:13 - train_loss: 1.12109 - train_acc 0.68750 - val_loss 1.07520 - val_acc 0.73438\n",
            "2020-08-19 11:07:30.582443 Epoch 2/3\n",
            "512/512 [============================>..] - ETA: 0:04:16 - train_loss: 0.81348 - train_acc 0.75000 - val_loss 0.85278 - val_acc 0.78906\n",
            "2020-08-19 12:11:36.067493 Epoch 3/3\n",
            "512/512 [============================>..] - ETA: 0:04:18 - train_loss: 0.68726 - train_acc 0.75000 - val_loss 0.76392 - val_acc 0.78906\n",
            "\n",
            "Elapsed time:  11521.919048786163\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-122296c4c94e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;31m# x_test = x_test[:256]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;31m# y_test = y_test[:256]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0mtensortype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPublicEncodedTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: "
          ]
        }
      ]
    }
  ]
}