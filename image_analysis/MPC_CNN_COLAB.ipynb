{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MPC CNN COLAB.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wb26w79iJCp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "import time\n",
        "from pond.tensor import NativeTensor, PublicEncodedTensor, PrivateEncodedTensor\n",
        "from pond.nn import Dense, Relu, Reveal, CrossEntropy, SoftmaxStable, Sequential, DataLoader, Conv2D, \\\n",
        "    AveragePooling2D, Flatten, BatchNorm, ReluNormal, ReluGalois, Conv2DQuant, DenseQuant\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# set errors error behaviour for overflow/underflow\n",
        "_ = np.seterr(over='raise')\n",
        "_ = np.seterr(under='raise')\n",
        "_ = np.seterr(invalid='raise')\n",
        "\n",
        "# %%\n",
        "\"\"\"\n",
        "- Load data from Keras\n",
        "- Split into training and test data\n",
        "- Normalise to [0,1] (greyscale images with values in [0,255])\n",
        "- Transform targets to one-hot representation\n",
        "\"\"\"\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "x_train = x_train.astype(np.uint8)\n",
        "x_test = x_test.astype(np.uint8)\n",
        "x_train = x_train[:, np.newaxis, :, :] / 255.0\n",
        "x_test = x_test[:, np.newaxis, :, :] / 255.0\n",
        "# x_train = x_train[:, np.newaxis, :, :]\n",
        "# x_test = x_test[:, np.newaxis, :, :]\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "\"\"\"\n",
        "Split into train-test-val\n",
        "Train in usual way using train and val\n",
        "Write separate function for testing on test set (completely unseen)\n",
        "Purpose of testing function is for evaluating quantized network\n",
        "Compare testing of quantized network with non-quantized network \n",
        "Need a way to save the parameters of the trained MPC network \n",
        "\"\"\"\n",
        "\n",
        "# Size of pooling area for max pooling\n",
        "# pool_size = (2, 2)\n",
        "# Convolution kernel size (kernel_height, kernel_width, input_channels, num_filters)\n",
        "# kernel_size = (5, 5, 1, 16)\n",
        "\n",
        "# convnet_shallow_gal = Sequential([\n",
        "#     #Conv2D((3, 3, 1, 32), strides=1, padding=1, filter_init=lambda shp: np.random.normal(scale=0.1, size=shp)),\n",
        "#     Conv2DQuant((3, 3, 1, 32), strides=1, padding=1, filter_init=lambda shp: np.random.normal(scale=0.1, size=shp)),\n",
        "#     BatchNorm(),\n",
        "#     ReluGalois(order=4, mu=0.0, sigma=1.0),     # TODO: investigate overflow error\n",
        "#     AveragePooling2D(pool_size=(2, 2)),\n",
        "#     Flatten(),\n",
        "#     # Dense(10, 6272),  # 3136 5408 6272\n",
        "#     DenseQuant(10, 6272),\n",
        "#     Reveal(),\n",
        "#     SoftmaxStable()\n",
        "# ])\n",
        "\n",
        "convnet_shallow = Sequential([\n",
        "    Conv2D((3, 3, 1, 32), strides=1, padding=1, filter_init=lambda shp: np.random.normal(scale=0.1, size=shp)),\n",
        "    BatchNorm(),\n",
        "    ReluNormal(order=4, mu=0.0, sigma=1.0, approx_type='regression'), # approx_type='taylor'),\n",
        "    # Relu(order=4),\n",
        "    AveragePooling2D(pool_size=(2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(10, 6272),  # 3136 5408 6272\n",
        "    Reveal(),\n",
        "    SoftmaxStable()\n",
        "])\n",
        "\n",
        "# convnet = Sequential([\n",
        "#     Conv2D((5, 5, 1, 1), strides=1, padding=1,\n",
        "#            filter_init=lambda shp: np.random.uniform(low=-0.14, high=0.14, size=shp)),\n",
        "#     # BatchNormTest(),\n",
        "#     # ReluNormal(order=3),\n",
        "#     AveragePooling2D(pool_size=(2,2)),\n",
        "#     Conv2D((5, 5, 1, 20), strides=1, padding=1,\n",
        "#            filter_init=lambda shp: np.random.uniform(low=-0.1, high=0.1, size=shp)),\n",
        "#     # BatchNormTest(),\n",
        "#     # ReluNormal(order=3),\n",
        "#     AveragePooling2D(pool_size=(2,2)),\n",
        "#     Flatten(),\n",
        "#     Dense(500, 500),\n",
        "#     BatchNormTest(),\n",
        "#     ReluNormal(order=3),\n",
        "#     Dense(10, 500),\n",
        "#     Reveal(),\n",
        "#     SoftmaxStable()\n",
        "# ])\n",
        "\n",
        "\n",
        "# %%\n",
        "\n",
        "def accuracy(classifier, x, y, verbose=0, wrapper=NativeTensor):\n",
        "    predicted_classes = classifier \\\n",
        "        .predict(DataLoader(x, wrapper), verbose=verbose).reveal() \\\n",
        "        .argmax(axis=1)\n",
        "\n",
        "    correct_classes = NativeTensor(y) \\\n",
        "        .argmax(axis=1)\n",
        "\n",
        "    matches = predicted_classes.unwrap() == correct_classes.unwrap()\n",
        "    return sum(matches) / len(matches)\n",
        "\n",
        "\n",
        "# %%\n",
        "\"\"\"\n",
        "Train on different types of Tensor\n",
        "\"\"\"\n",
        "# NativeTensor (like plaintext)\n",
        "x_train = x_train[:64]\n",
        "y_train = y_train[:64]\n",
        "x_test = x_test[:32]\n",
        "y_test = y_test[:32]\n",
        "\n",
        "tensortype = PrivateEncodedTensor  # TODO: Change back to NativeTensor\n",
        "batch_size = 32\n",
        "input_shape = [batch_size] + list(x_train.shape[1:])\n",
        "epochs = 3\n",
        "learning_rate = 0.01\n",
        "\n",
        "convnet_shallow.initialize(initializer=tensortype, input_shape=input_shape)\n",
        "\n",
        "start = time.time()\n",
        "convnet_shallow.fit(\n",
        "    x_train=DataLoader(x_train, wrapper=tensortype),\n",
        "    y_train=DataLoader(y_train, wrapper=tensortype),\n",
        "    x_valid=DataLoader(x_test, wrapper=tensortype),\n",
        "    y_valid=DataLoader(y_test, wrapper=tensortype),\n",
        "    loss=CrossEntropy(),\n",
        "    epochs=epochs,\n",
        "    batch_size=batch_size,\n",
        "    verbose=1,\n",
        "    learning_rate=learning_rate\n",
        ")\n",
        "end = time.time()\n",
        "time_taken = end - start\n",
        "\n",
        "print(\"Elapsed time: \", time_taken)\n",
        "# print(\"Train accuracy:\", accuracy(convnet, x_train, y_train))\n",
        "# print(\"Test accuracy:\", accuracy(convnet, x_test, y_test))\n",
        "\n",
        "# %%\n",
        "# PublicEncodedTensor (MPC operations on public values, i.e. unencrypted)\n",
        "# Can train on subset of batches due to long training times\n",
        "# x_train = x_train[:256]\n",
        "# y_train = y_train[:256]\n",
        "# x_test = x_test[:256]\n",
        "# y_test = y_test[:256]\n",
        "raise Exception()\n",
        "tensortype = PublicEncodedTensor\n",
        "epochs = 1\n",
        "learning_rate = 0.01\n",
        "\n",
        "convnet_shallow.initialize(initializer=tensortype, input_shape=input_shape)\n",
        "\n",
        "start = time.time()\n",
        "convnet_shallow.fit(\n",
        "    x_train=DataLoader(x_train, wrapper=tensortype),\n",
        "    y_train=DataLoader(y_train, wrapper=tensortype),\n",
        "    x_valid=DataLoader(x_test, wrapper=tensortype),\n",
        "    y_valid=DataLoader(y_test, wrapper=tensortype),\n",
        "    loss=CrossEntropy(),\n",
        "    epochs=epochs,\n",
        "    batch_size=batch_size,\n",
        "    verbose=1,\n",
        "    learning_rate=learning_rate\n",
        ")\n",
        "end = time.time()\n",
        "time_taken = end - start\n",
        "\n",
        "print(\"Elapsed time: \", time_taken)\n",
        "# print(\"Train accuracy:\", accuracy(convnet, x_train, y_train))\n",
        "# print(\"Test accuracy:\", accuracy(convnet, x_test, y_test))\n",
        "\n",
        "# %%\n",
        "# PrivateEncodedTensor (full MPC)\n",
        "x_train = x_train[:256]\n",
        "y_train = y_train[:256]\n",
        "x_test = x_test[:256]\n",
        "y_test = y_test[:256]\n",
        "\n",
        "tensortype = PrivateEncodedTensor\n",
        "epochs = 1\n",
        "learning_rate = 0.01\n",
        "\n",
        "convnet_shallow.initialize(initializer=tensortype, input_shape=input_shape)\n",
        "\n",
        "start = time.time()\n",
        "convnet_shallow.fit(\n",
        "    x_train=DataLoader(x_train, wrapper=tensortype),\n",
        "    y_train=DataLoader(y_train, wrapper=tensortype),\n",
        "    x_valid=DataLoader(x_test, wrapper=tensortype),\n",
        "    y_valid=DataLoader(y_test, wrapper=tensortype),\n",
        "    loss=CrossEntropy(),\n",
        "    epochs=epochs,\n",
        "    batch_size=batch_size,\n",
        "    verbose=1,\n",
        "    learning_rate=learning_rate\n",
        ")\n",
        "end = time.time()\n",
        "time_taken = end - start\n",
        "\n",
        "print(\"Elapsed time: \", time_taken)\n",
        "# print(\"Train accuracy:\", accuracy(convnet, x_train, y_train))\n",
        "# print(\"Test accuracy:\", accuracy(convnet, x_test, y_test))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}